<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tianye Li</title>
  
  <meta name="author" content="Tianye Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        
        <!-- Intro -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tianye Li</name>
              </p>
              <p>I am a <a href="http://phdcomics.com/">Ph.D.</a> candidate in <a href="https://www.cs.usc.edu/">Computer Science</a> at <a href="https://www.usc.edu/">USC</a>, advised by <a href="http://www.hao-li.com/Hao_Li/Hao_Li_-_about_me.html">Prof. Hao Li</a> and <a href="https://ict.usc.edu/profile/randall-hill-jr/">Prof. Randall Hill, Jr.</a> I also work in the <a href="https://vgl.ict.usc.edu/">Vision and Graphics Lab</a> at <a href="https://ict.usc.edu/">USC Institute for Creative Technologies</a>.
              </p>
              <p>
                In summer and fall 2020, I am a research intern at the <a href="https://research.fb.com/category/augmented-reality-virtual-reality/">Facebook Reality Labs</a>, hosted by <a href="https://lvzhaoyang.github.io/">Zhaoyang Lv</a>. 
                In summer 2018, I was a research intern at <a href="https://www.snap.com/en-US/">Snap Inc.</a>, working with <a href="http://chongyangma.com/">Chongyang Ma</a> and <a href="http://linjieluo.com/">Linjie Luo</a>. From fall 2016 to spring 2017, I visited the <a href="https://ps.is.tuebingen.mpg.de/">Max Planck Institute for Intelligent Systems</a> (<a href="https://en.wikipedia.org/wiki/T%C3%BCbingen">TÃ¼bingen</a>), working with <a href="https://sites.google.com/site/bolkartt/">Timo Bolkart</a>, <a href="https://scholar.google.com/citations?user=Wx62iOsAAAAJ&hl=en">Javier Romero</a>, and <a href="https://ps.is.tuebingen.mpg.de/person/black">Michael J. Black</a>.
              </p>
              <p>
                I did my B.Eng. at <a href="http://en.xidian.edu.cn/">Xidian University</a> and M.Sc. (with honor) at <a href="https://minghsiehece.usc.edu/">USC</a>, both in Electrical Engineering, during which I had worked in <a href="https://www.keysight.com/us/en/home.html">Agilent Technologies (Keysight Technologies)</a> and <a href="https://www.dolby.com/technologies/dolby-vision/">Dolby Laboratories</a>.
              </p>
              <p style="text-align:center">
                Email: &lt;first_name&gt; &lt;last_name&gt; <i>at</i> protonmail <i>dot</i> com
              </p>
              <p style="text-align:center">
                <a href="data/Tianye_Li_full_cv_20210305.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=ztIh4rgAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/_TianyeLi">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/tianyeli.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/tianyeli.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- Research -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests are computer vision and computer graphics. Most of my work aim to capture and analyze the geometry, motion and appearance of dynamic and deformable objects, including human face and body as well as general objects and scenes.
                <!-- Representative papers are <span class="highlight">highlighted</span>. -->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <!-- Arxiv 2021 neural 3d video -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/arxiv21_neural_3d_video.png" alt="arxiv21" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <td width="75%" valign="middle"> -->
              <a href="https://arxiv.org/pdf/2103.02597.pdf">
                <papertitle>Neural 3D Video Synthesis</papertitle>
              </a>
              <br>
              <strong>Tianye Li</strong>*,
              <a href="https://scholar.google.de/citations?user=cJKcPcIAAAAJ&hl=en">Mira Slavcheva</a>*,
              <a href="https://zollhoefer.com/">Michael Zollhoefer</a>,
              <a href="https://scholar.google.com/citations?user=VmHZtsEAAAAJ&hl=en">Simon Green</a>,
              <a href="https://christophlassner.de/">Christoph Lassner</a>,
              <a href="https://changilkim.com/">Changil Kim</a>,
              <a href="https://tschmidt23.github.io/">Tanner Schmidt</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=JVum8voAAAAJ">Steven Lovegrove</a>,
              <a href="https://scholar.google.com/citations?user=56UhAooAAAAJ&hl=en">Michael Goesele</a>,
              <a href="https://lvzhaoyang.github.io/">Zhaoyang Lv</a>
              <br>
              <em>ArXiv 2021</em>
              <br>
              <a href="https://arxiv.org/abs/2103.02597">arxiv</a> /
              <a href="https://neural-3d-video.github.io/">project page</a> /
              <a href="./bibtex/arxiv21_neural_3d_video.txt">bibtex</a>
              <p>A method that captures complex dynamic scenes and enables photorealistic 3D video synthesis from wide view angles and at arbitary times.</p>
            </td>
          </tr>

          <!-- TPAMI 2020 differentiable rendering -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tpami20_softras.png" alt="tpami20" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <td width="75%" valign="middle"> -->
              <a href="https://ieeexplore.ieee.org/abstract/document/9134794">
                <papertitle>A General Differentiable Mesh Renderer for Image-based 3D Reasoning</papertitle>
              </a>
              <br>
              <a href="https://shichenliu.github.io/">Shichen Liu</a>,
              <strong>Tianye Li</strong>,
              <a href="http://chenweikai.github.io/">Weikai Chen</a>,
              <a href="http://www.hao-li.com/Hao_Li/Hao_Li_-_about_me.html">Hao Li</a>
              <br>
              <em>TPAMI 2020</em>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9134794">paper</a> /
              <a href="./bibtex/tpami20_softras.txt">bibtex</a>
              <p>An extended version of the <i>Soft Rasterizer</i>.</p>
            </td>
          </tr>

          <!-- ICCV 2019 differentiable rendering -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccv19_softras.png" alt="iccv19_softras" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <td width="75%" valign="middle"> -->
              <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Soft_Rasterizer_A_Differentiable_Renderer_for_Image-Based_3D_Reasoning_ICCV_2019_paper.pdf">
                <papertitle>Soft Rasterizer: A Differentiable Renderer for Image-based 3D Reasoning</papertitle>
              </a>
              <br>
              <a href="https://shichenliu.github.io/">Shichen Liu</a>,
              <strong>Tianye Li</strong>,
              <a href="http://chenweikai.github.io/">Weikai Chen</a>,
              <a href="http://www.hao-li.com/Hao_Li/Hao_Li_-_about_me.html">Hao Li</a>
              <br>
              <em>ICCV 2019 <font color="red"><strong>(Oral Presentation)</strong></font></em>
              <br>
              <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Soft_Rasterizer_A_Differentiable_Renderer_for_Image-Based_3D_Reasoning_ICCV_2019_paper.pdf">paper</a> /
              <a href="https://github.com/ShichenLiu/SoftRas">code</a> /
              <a href="./bibtex/iccv19_softras.txt">bibtex</a>
              <p>A rasterization-based differentiable renderer for 3D meshes, <i>Soft Rasterizer (SoftRas)</i>, that supports reasoning for geometry, texture, lighting conditions and camera poses with 2D images. An extended version of <i>SoftRas</i> has been incorporated into the <a href="https://pytorch3d.org/">PyTorch3D</a> library.</p>
            </td>
          </tr>

          <!-- ICCV 2019 face protrait undistortion -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccv19_protrait_undist.png" alt="iccv19_protrait_undist" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <td width="75%" valign="middle"> -->
              <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Learning_Perspective_Undistortion_of_Portraits_ICCV_2019_paper.pdf">
                <papertitle>Learning Perspective Undistortion of Portraits</papertitle>
              </a>
              <br>
              <a href="http://ict.usc.edu/profile/yajie-zhao/">Yajie Zhao</a>,
              <a href="https://zeng.science/">Zeng Huang</a>,
              <strong>Tianye Li</strong>,
              <a href="http://chenweikai.github.io/">Weikai Chen</a>,
              <a href="http://www.chloelegendre.com/">Chloe LeGendre</a>,
              <a href="">Xinglei Ren</a>,
              <a href="http://junxnui.github.io/">Jun Xing</a>,
              <a href="http://www.arishapiro.com/">Ari Shapiro</a>,
              <a href="http://www.hao-li.com/Hao_Li/Hao_Li_-_about_me.html">Hao Li</a>
              <br>
              <em>ICCV 2019 <font color="red"><strong>(Oral Presentation)</strong></font></em>
              <br>
              <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Learning_Perspective_Undistortion_of_Portraits_ICCV_2019_paper.pdf">paper</a> /
              <a href="https://github.com/bearjoy730/Learning-Perspective-Undistortion-of-Portraits">project page</a> /
              <a href="./bibtex/iccv19_protrait_undist.txt">bibtex</a>
              <p>Given a face portrait, this system corrects the perspective distortion, easing subsequent facial recognition and reconstruction and reducing the bias for human perception.</p>
            </td>
          </tr>

          <!-- ECCV 2018 sparse view voluemtric capture -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/eccv18_volume_capture.png" alt="eccv18" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <td width="75%" valign="middle"> -->
              <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zeng_Huang_Deep_Volumetric_Video_ECCV_2018_paper.pdf">
                <papertitle>Deep Volumetric Video from Very Sparse Multi-view Performance Capture</papertitle>
              </a>
              <br>
              <a href="https://zeng.science/">Zeng Huang</a>,
              <strong>Tianye Li</strong>,
              <a href="http://chenweikai.github.io/">Weikai Chen</a>,
              <a href="http://ict.usc.edu/profile/yajie-zhao/">Yajie Zhao</a>,
              <a href="http://junxnui.github.io/">Jun Xing</a>,
              <a href="http://www.chloelegendre.com/">Chloe LeGendre</a>,
              <a href="http://linjieluo.com/">Linjie Luo</a>,
              <a href="http://chongyangma.com/">Chongyang Ma</a>,
              <a href="http://www.hao-li.com/Hao_Li/Hao_Li_-_about_me.html">Hao Li</a>
              <br>
              <em>ECCV 2018</em>
              <br>
              <a href="https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnx0aWFueWVmb2N1c3xneDoxMzg5OTQwODEzOTZmYjQ3">paper</a> /
              <a href="https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnx0aWFueWVmb2N1c3xneDo2ZjViNDQ5Nzc5Y2NmMzg2">supplemental</a> /
              <a href="https://www.youtube.com/watch?v=sQnXlQ3GyKc&feature=youtu.be">video</a> /
              <a href="https://www.dropbox.com/s/kl73b80xhe3bic3/ECCV18_Poster_final.pdf?dl=0">poster</a> /
              <a href="./bibtex/eccv18_volume_capture.txt">bibtex</a>
              <p>Utilizing a learnt implicit representation for geometry, the method is able to capture dynamic performances of human actors with very sparse camera settings (3 or 4 views), which enables to high-quality volumetric videos.</p>
            </td>
          </tr>

          <!-- SIGGRAPH Asia 2017 FLAME face model -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sigasia17_flame.png" alt="sigasia17" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <td width="75%" valign="middle"> -->
              <a href="https://dl.acm.org/doi/10.1145/3130800.3130813">
                <papertitle>Learning a Model of Facial Shape and Expression from 4D Scans</papertitle>
              </a>
              <br>
              <strong>Tianye Li</strong>*,
              <a href="https://sites.google.com/site/bolkartt/">Timo Bolkart</a>*,
              <a href="https://ps.is.tuebingen.mpg.de/person/black/">Michael J. Black</a>,
              <a href="http://www.hao-li.com/Hao_Li/Hao_Li_-_about_me.html">Hao Li</a>,
              <a href="https://ps.is.tuebingen.mpg.de/person/jromero">Javier Romero</a>
              <br>
              <em>SIGGRAPH Asia 2017</em>
              <br>
              <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/400/paper.pdf">paper</a> /
              <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/401/supplementary.pdf">supplemental</a> /
              <a href="https://youtu.be/36rPTkhiJTM">video</a> /
              <a href="https://youtu.be/kslTkzBKR1Q">fast-forward</a> /
              <a href="http://flame.is.tue.mpg.de/">project page</a> /
              <a href="http://flame.is.tue.mpg.de/">model & data</a> /
              <a href="./bibtex/sigasia17_flame.txt">bibtex</a>
              <p>We propose a light-weight yet expressive generic face model, <i>FLAME</i>, by learning from large high-quality datasets and an appropriate separation of identity, expression and pose. The <i>FLAME</i> model has been incorporated into the <a href="https://smpl-x.is.tue.mpg.de/"><i>SMPL-X</i></a> model.
              </p>
            </td>
          </tr>

          <!-- ECCV 2016 face tracking -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/eccv16_face_track.png" alt="eccv16" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <!-- <td width="75%" valign="middle"> -->
              <a href="https://link.springer.com/chapter/10.1007/978-3-319-46484-8_15">
                <papertitle>Real-Time Facial Segmentation and Performance Capture from RGB Input</papertitle>
              </a>
              <br>
              <a href="http://www-scf.usc.edu/~saitos/">Shunsuke Saito</a>,
              <strong>Tianye Li</strong>,
              <a href="http://www.hao-li.com/Hao_Li/Hao_Li_-_about_me.html">Hao Li</a>
              <br>
              <em>ECCV 2016</em>
              <br>
              <a href="http://www.hao-li.com/publications/papers/eccv2016RTFSPCRI.pdf">paper</a> /
              <a href="http://www.hao-li.com/publications/additionalMaterials/eccv2016additionalMaterials.pdf">supplemental</a> /
              <a href="http://arxiv.org/pdf/1604.02647v1.pdf">arxiv</a> /
              <a href="https://www.youtube.com/watch?v=hPksv1gJet4&feature=youtu.be">video</a> /
              <a href="http://www.eccv2016.org/files/posters/P-3C-13.pdf">poster</a> /
              <a href="https://drive.google.com/file/d/0B0J5iCVLyQO9UlhfbEhtOW54cTQ/view">data</a> /
              <a href="./bibtex/eccv16_face_track.txt">bibtex</a>
              <p>A real-time facial performance capture system from single RGB camera, that is robust to occlusion, thanks to an effective and real-time facial segmentation network.</p>
            </td>
          </tr>

        <!-- Service -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          
          <!-- Review -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/reviewer_meme.png" alt="reviewer" width="160" style="border-style: none"></td>
            <td width="75%" valign="center">
              Reviewer <ul>
                <li>CVPR (2020, 2021), ICCV (2019, 2021), ECCV (2020), TPAMI (2021), IJCV (2020), NeurIPS (2020), AAAI (2020), VRST (2020), Eurographics (2019), Pacific Graphics (2018), CAVW (2018), ICCV Workshop PeopleCap (2017), IEEE VR (2017)</li>
              </ul>
              <!-- Reviewer <ul>
                <li>NeurIPS 2020, AAAI 2020</li>
                <li>ECCV 2020, CVPR 2020, ICCV 2019, ICCV Workshop PeopleCap 2017</li>
                <li>VRST 2020, Eurographics 2019, Pacific Graphics 2018, CAVW 2018, IEEE VR 2017</li>
              </ul> -->
              <a href="http://cvpr2020.thecvf.com/reviewer-acknowledgements"><i>Outstanding Reviewer, CVPR 2020</i></a>
            </td>
          </tr>

          <!-- Teaching -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/trojan_logo.jpeg" alt="trojan" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="center">
              Teaching Assistant, CSCI 677 Advanced Computer Vision, Fall 2019
              <br>
              <br>
              Teaching Assistant, <a href="http://www.hao-li.com/Hao_Li/Hao_Li_-_teaching_%5BCSCI_621__Digital_Geometry_Processing_SS_2018%5D.html">CSCI 621 Digital Geometry Processing</a>, Spring 2018
              <br>
              <br>
              Grader, EE 559 Mathematical Pattern Recognition, Spring 2015
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center">
                Latest update: March 5, 2021.
                <br>
                Wonderful template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
